{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python380jvsc74a57bd070d26f2264f7fe31f4c81ee0d745621efd984d696ceda4ad32e9609dbdcfd161",
   "display_name": "Python 3.8.0 64-bit"
  },
  "metadata": {
   "interpreter": {
    "hash": "70d26f2264f7fe31f4c81ee0d745621efd984d696ceda4ad32e9609dbdcfd161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Nightlight Satellite Image Export\n",
    "## Prerequisites\n",
    "### Input\n",
    "\n",
    "* CSV with 4 columns:\n",
    "    * ID\n",
    "    * year of survey\n",
    "    * Latitude-Coordinates\n",
    "    * Longitude-Coordinates\n",
    "\n",
    "### Execution\n",
    "#### main pipeline\n",
    "Should work like this:\n",
    "\n",
    "1. create a pandas dataframe with all the information and delete all unnecessary entries (in our case it is all entries who are from 2011 or earlier)\n",
    "2. creation of a range of year in string format (yyyy-mm-dd) instead a year in integer format\n",
    "3. create an image with the median\n",
    "4. get the areas of interest (AOI) with the Latitude and Longitude information in the dataframe\n",
    "5. export the image with predefined AOIs and dimensions\n",
    "    \n",
    "Google Documentary says 'To get a block of pixels of predefined size (for example a 256x256 thumbnail image) that covers a region, specify dimensions and region'\n",
    "\n",
    "#### Alternative pipeline (same as in the africa-poverty paper)\n",
    "1. create a pandas dataframe with all the information and delete all unnecessary entries (in our case it is all entries who are from 2011 or earlier)\n",
    "2. creation of a range of year in string format (yyyy-mm-dd) instead a year in integer format\n",
    "3. create a Featurecollection with all the information needed to get the correct images (Area of interest with a radius of 5.5 km)\n",
    "4. create an image with the Featurecollection and the median\n",
    "5. export them to google drive (there is no need for Authentication, it gets exported to the same account used for google earth engine)\n",
    "6. create a function which downloads all images instantly as long as there are not the same amount of images in the drive compared to the length of the dataframe"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ee\n",
    "import pandas as pd\n",
    "from typing import Tuple\n",
    "import utils\n",
    "import geemap #Nur fÃ¼r Visualisierungzwecke im jetzigen Zustand, wird beim Export der Daten wieder entfernt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ee.Authenticate()\n",
    "ee.Initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONSTANTS\n",
    "#Path of CSV-File with columns ID,year,LATITUDE,LONGITUDE\n",
    "csvpath = '../dataResearch/firstSample.csv'\n",
    "#name all column names\n",
    "#year\n",
    "year = 'year'\n",
    "#Latitude and Longitude Coordinates\n",
    "LATNUM = 'LATNUM'\n",
    "LONGNUM= 'LONGNUM'\n",
    "#ID for Filenames\n",
    "surveyid = 'ID'\n",
    "#Export parameters\n",
    "DHS_EXPORT_FOLDER = 'dhs_geotiff_raw'\n",
    "# image export parameters\n",
    "PROJECTION = 'EPSG:3857'  # see https://epsg.io/3857\n",
    "#1100 pixel times 10m equals 11000mx11000m image\n",
    "SCALE = 10                # export resolution: 10m/px\n",
    "IMAGE_DIMENSION = 5500  #radius of the wanted image in m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create Dataframe with pandas\n",
    "df = pd.read_csv(csvpath)\n",
    "#create Dataframe where year is 2012 or higher\n",
    "df = df[df.year >= 2012]\n",
    "for i in range(len(df)):\n",
    "    start_date,end_date = utils.surveyyear_to_range(df[year].iloc[i],satellitename='nl')\n",
    "    # get the VIIRS image collection, we're using the \"avg_rad\" band\n",
    "    dataset = ee.ImageCollection('NOAA/VIIRS/DNB/MONTHLY_V1/VCMCFG').filter(ee.Filter.date(start_date, end_date))\n",
    "    nighttime = dataset.select('avg_rad').median()\n",
    "    # get the coordinates based on the AOI\n",
    "    lat = float(df[LATNUM].iloc[i].replace(',', '.'))\n",
    "    lon = float(df[LONGNUM].iloc[i].replace(',', '.'))\n",
    "    aoi = ee.Geometry.Point(lon,lat)\n",
    "    coords = utils.point_to_box_coords(aoi=aoi,dimensionradius=IMAGE_DIMENSION)\n",
    "    name = df[surveyid].iloc[i]\n",
    "    task = ee.batch.Export.image.toDrive(image=nighttime,region=coords,folder=DHS_EXPORT_FOLDER,fileNamePrefix=name,scale=SCALE,description=name)\n",
    "    task.start()"
   ]
  },
  {
   "source": [
    "## Export from Google Drive to Sciebo"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Enable Google Drive\n",
    "#needs credentials.json if used for the first use or token.json if used for recurrent uses\n",
    "#import libraries\n",
    "import os.path\n",
    "from googleapiclient.discovery import build\n",
    "from google_auth_oauthlib.flow import InstalledAppFlow\n",
    "from google.auth.transport.requests import Request\n",
    "from google.oauth2.credentials import Credentials\n",
    "\n",
    "creds = None\n",
    "# The file token.json stores the user's access and refresh tokens, and is\n",
    "# created automatically when the authorization flow completes for the first\n",
    "# time.\n",
    "if os.path.exists('token.json'):\n",
    "    creds = Credentials.from_authorized_user_file('token.json',['https://www.googleapis.com/auth/drive'])\n",
    "# If there are no (valid) credentials available, let the user log in.\n",
    "if not creds or not creds.valid:\n",
    "    if creds and creds.expired and creds.refresh_token:\n",
    "        creds.refresh(Request())\n",
    "    else:\n",
    "        flow = InstalledAppFlow.from_client_secrets_file(\n",
    "            'credentials.json', ['https://www.googleapis.com/auth/drive'])\n",
    "        creds = flow.run_local_server(port=0)\n",
    "    # Save the credentials for the next run\n",
    "    with open('token.json', 'w') as token:\n",
    "        token.write(creds.to_json())\n",
    "\n",
    "service = build('drive', 'v3', credentials=creds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file(file_id, filename):\n",
    "    request = service.files().get_media(fileId=file_id)\n",
    "    fh = io.FileIO(filename, 'wb')\n",
    "    downloader = MediaIoBaseDownload(fh, request)\n",
    "    done = False\n",
    "    while done is False:\n",
    "        status, done = downloader.next_chunk()\n",
    "        print('Download done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "from googleapiclient.http import MediaIoBaseDownload\n",
    "# Call the Drive v3 API\n",
    "dirpath = '../data'\n",
    "os.makedirs(dirpath,exist_ok=True)\n",
    "while len([name for name in os.listdir(dirpath)]) != len(df):\n",
    "    results = service.files().list(q=\"mimeType='image/tiff'\",spaces='drive',\n",
    "                                          fields='nextPageToken, files(id, name)',\n",
    "                                          pageToken=None).execute()\n",
    "    items = results.get('files', [])\n",
    "    for item in items:\n",
    "        file_id = item.get('id')\n",
    "        filename = item.get('name')\n",
    "        print(\"Download \" + str(filename))\n",
    "        download_file(item['id'], item['name'])\n",
    "        service.files().delete(fileId=file_id).execute()\n",
    "        os.replace(filename,dirpath + \"/\" + filename)\n",
    "        print(str(len([name for name in os.listdir(dirpath)]))+ \"/\" + str(len(df)))"
   ]
  }
 ]
}