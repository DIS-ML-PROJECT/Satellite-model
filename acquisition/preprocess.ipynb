{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": []
  },
  {
   "source": [
    "# To-Dos in diesem Notebook\n",
    "- Nachdenken über Deletion von alten Daten (year<2013)\n",
    "- Exportprobleme beheben (kein Export nach Sciebo möglich, Earth Engine unterstützt nur Drive, Google Cloud Storage und Earth Engine Arrays)\n",
    "- Preprocess muss vor dem Export weitergehen\n",
    "    - Normalisieren der Daten\n",
    "    - Interpolation muss m.M.n. nicht gemacht werden, bzw. wird durch die Pixelangabe schon gemacht"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites\n",
    "\n",
    "The images are saved in gzipped TFRecord format. By default, this notebook exports images to Google Drive. If you instead prefer to export images to Google Cloud Storage (GCS), change the `EXPORT` constant below to `'gcs'` and set `BUCKET` to the desired GCS bucket name.\n",
    "\n",
    "## Expected Storage Size of Landsat data\n",
    "\n",
    "| Survey | Storage  | Expected Export Time |\n",
    "| :-: | :-: | :-: |\n",
    "| DHS  | ~16.0 GB | ~24h |\n",
    "| LSMS |  ~2.5 GB | ~10h |\n",
    "\n",
    "## Expected Storage Size of Landsat Data:\n",
    "### Sentinel imagery has 3 times higher resolution than Landsat imagery\n",
    "\n",
    "| Survey | Storage  | Expected Export Time |\n",
    "| :-: | :-: | :-: |\n",
    "| DHS  | ~48.0 GB | ~72h\n",
    "| LSMS |  ~7.5 GB | ~30h\n",
    "\n",
    "\n",
    "The folder structure should look as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "    dhs_tfrecords_raw/\n",
    "        angola_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        zimbabwe_2015_00.tfrecord.gz\n",
    "    lsms_tfrecords_raw/\n",
    "        ethiopia_2011_00.tfrecord.gz\n",
    "        ...\n",
    "        uganda_2013_00.tfrecord.gz\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import ee\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Authenticate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ee.Initialize()  # initialize the Earth Engine API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== ADAPT THESE PARAMETERS ==========\n",
    "\n",
    "# To export to Google Drive, uncomment the next 2 lines\n",
    "EXPORT = 'drive'\n",
    "BUCKET = None\n",
    "\n",
    "# To export to Google Cloud Storage (GCS), uncomment the next 2 lines\n",
    "# and set the bucket to the desired bucket name\n",
    "# EXPORT = 'gcs'\n",
    "# BUCKET = 'mybucket'\n",
    "\n",
    "# export location parameters\n",
    "DHS_EXPORT_FOLDER = 'dhs_tfrecords_raw'\n",
    "LSMS_EXPORT_FOLDER = 'lsms_tfrecords_raw'"
   ]
  },
  {
   "source": [
    "## Mögliche Bands\n",
    "- 10 m/px\n",
    "    - 'BLUE' (B2)\n",
    "    - 'GREEN' (B3)\n",
    "    - 'RED' (B4)\n",
    "    - 'NIR' (B8)\n",
    "- 20 m/px\n",
    "    - 'RED EDGE' (B5)\n",
    "    - 'NIR' (B6,B7,B8A)\n",
    "    - 'SWIR' (B11,B12)\n",
    "- 60 m/px\n",
    "    - 'COASTAL AEROSOL' (B1)\n",
    "    - 'CIRRUS' (B10)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input data paths\n",
    "DHS_CSV_PATH = '../data/dhs_wealthindex.csv'\n",
    "#LSMS_CSV_PATH = '../data/lsms_clusters.csv'\n",
    "\n",
    "# band names\n",
    "MS_BANDS = ['BLUE', 'GREEN', 'RED', 'NIR', 'SWIR1', 'SWIR2', 'TEMP1']\n",
    "\n",
    "# image export parameters\n",
    "SCALE = 10                # export resolution: 10m/px\n",
    "EXPORT_TILE_RADIUS = 335  # image dimension = (2*EXPORT_TILE_RADIUS) + 1 = 671px = 6710m\n",
    "CHUNK_SIZE = 1000         # set to a small number (<= 50) if Google Earth Engine reports memory errors, may have to talk about that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_images(\n",
    "        df: pd.DataFrame,\n",
    "        country: str,\n",
    "        year: int,\n",
    "        export_folder: str,\n",
    "        chunk_size: Optional[int] = None,\n",
    "        ) -> Dict[Tuple[Any], ee.batch.Task]:\n",
    "    '''\n",
    "    Args\n",
    "    - df: pd.DataFrame, contains columns ['lat', 'lon', 'country', 'year']\n",
    "    - country: str, together with `year` determines the survey to export\n",
    "    - year: int, together with `country` determines the survey to export\n",
    "    - export_folder: str, name of folder for export\n",
    "    - chunk_size: int, optionally set a limit to the # of images exported per TFRecord file\n",
    "        - set to a small number (<= 50) if Google Earth Engine reports memory errors\n",
    "\n",
    "    Returns: dict, maps task name tuple (export_folder, country, year, chunk) to ee.batch.Task\n",
    "    '''\n",
    "    subset_df = df[(df['country'] == country) & (df['year'] == year)].reset_index(drop=True)\n",
    "    if chunk_size is None:\n",
    "        num_chunks = 1\n",
    "    else:\n",
    "        num_chunks = int(math.ceil(len(subset_df) / chunk_size))\n",
    "    tasks = {}\n",
    "\n",
    "    for i in range(num_chunks):\n",
    "        chunk_slice = slice(i * chunk_size, (i+1) * chunk_size - 1)  # df.loc[] is inclusive\n",
    "        fc = ee_utils.df_to_fc(subset_df.loc[chunk_slice, :])\n",
    "        start_date, end_date = ee_utils.surveyyear_to_range(year)\n",
    "\n",
    "        # SENTINEL image here instead of LandsatSR, including Cloudmasking\n",
    "        #roi = fc.geometry()\n",
    "        #imgcol = ee_utils.LandsatSR(roi, start_date=start_date, end_date=end_date).merged\n",
    "        #imgcol = imgcol.select(MS_BANDS)\n",
    "        #img = imgcol.median()\n",
    "\n",
    "        # add nightlights, latitude, and longitude bands\n",
    "        #img = ee_utils.add_latlon(img)\n",
    "        #img = img.addBands(ee_utils.composite_nl(year))\n",
    "\n",
    "        fname = f'{country}_{year}_{i:02d}'\n",
    "        #Export doesnt work here as planned, ee only supports export to Drive, GCS and Earth Engine Arrays\n",
    "            #tasks[(export_folder, country, year, i)] = ee_utils.get_array_patches(\n",
    "                #img=img, scale=SCALE, ksize=EXPORT_TILE_RADIUS,\n",
    "                #points=fc, export='drive',\n",
    "                #prefix=export_folder, fname=fname,\n",
    "                #bucket=None)\n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagery Download based on LSMS Survey\n",
    "#doesn't work right now because of Export Problems\n",
    "dhs_df = pd.read_csv(DHS_CSV_PATH, float_precision='high', index_col=False)\n",
    "dhs_surveys = list(dhs_df.groupby(['country', 'year']).groups.keys())\n",
    "tasks = {}\n",
    "\n",
    "for country, year in dhs_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=dhs_df, country=country, year=year,\n",
    "        export_folder=DHS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imagery Download based on LSMS Survey\n",
    "#doesn't work right now because of Lack of LSMS Survey Data and Export Problems\n",
    "lsms_df = pd.read_csv(LSMS_CSV_PATH, float_precision='high', index_col=False)\n",
    "lsms_surveys = list(lsms_df.groupby(['country', 'year']).groups.keys())\n",
    "\n",
    "for country, year in lsms_surveys:\n",
    "    new_tasks = export_images(\n",
    "        df=lsms_df, country=country, year=year,\n",
    "        export_folder=LSMS_EXPORT_FOLDER, chunk_size=CHUNK_SIZE)\n",
    "    tasks.update(new_tasks)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python380jvsc74a57bd070d26f2264f7fe31f4c81ee0d745621efd984d696ceda4ad32e9609dbdcfd161",
   "display_name": "Python 3.8.0 64-bit"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "metadata": {
   "interpreter": {
    "hash": "70d26f2264f7fe31f4c81ee0d745621efd984d696ceda4ad32e9609dbdcfd161"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}